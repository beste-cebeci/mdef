## “Echoes of Earth” Project Development Process Reflection

![artifact video](../../images/artifactvideo.mp4)

[project link: https://www.hackster.io/544134/echoes-of-earth-099b88](https://www.hackster.io/544134/echoes-of-earth-099b88)

I am really proud of the project we developed. Even though it was hard to merge our personal projects in some ways at the beginning, we all had the same vision of what we wanted this artifact to be. But I guess it was too broad or we couldn’t word it well in the first presentation. I always find it hard to describe what I have in mind or what I develop during the process and explain it with words. I do believe that the outcome speaks for itself. I know for this class it was important to share our struggles and problem-solving for others and for ourselves, to help us in future projects.
Echoes of Earth is an artifact that translates what is underneath the soil by reading soil moisture data and influencing a real-time root visual and sound. It also has LEDs to resemble fireflies, which ties the whole concept together by mimicking them in showcasing the health and condition of the soil.

When we were ideating, one of the references resembled fireflies to me, and we all immediately started sharing our own stories of seeing them and how magical those moments felt. We didn’t continue with it while developing the concept and the tech, but it somehow came back and completed the narrative of our project.

This is definitely a work in progress. It has the potential to become more. Currently, it is a human-centric device that only reflects one single data. More sensors can be added, but also an action can be generated for it to expand the connection with earth that it creates.

![artifact video](../../images/eoe.jpg)

In my personal project, I will try to figure out ways to scan scars left on the environment . So learning about GPR and seeing what sensors we have felt like first steps toward that as well. Becoming aware of public datasets also gave me some ideas.
I also really enjoyed working on this project because we were fewer people in the group and were able to divide the workload based on our interests and strengths. I worked on the digital visual output of the project in TouchDesigner. I wouldn’t say it’s my strength because I haven’t worked with it before, except for generating audio for another project. The visualization part and incorporating data were all new to me. So I followed lots of tutorials, mixed and matched their workflows, and Armin helped me a lot along the way.

![artifact video](../../images/eoe_td.png)

We ended up with a working skeleton, which didn’t work properly during the presentation, so it wasn’t as effective. When deciding what the visual should look like, I stuck with the idea that we wanted to show what we cannot see under the soil. I took reference from one of the root images in the datasets we found and tried to add movement and abstract it without losing too much detail. It still needs more tweaks and experimentation, but right now, as the sensor goes deeper into the soil, the particles come together from a scattered stage and form the root image. The data is also linked to base frequency levels, and the audio changes with different moisture levels, which supports the impact of the experience.

This project was possible thanks to the FabLab team. I appreciate their help, references and feedback!

Datasets mentioned: 
https://www.wur.nl/en/library/image-collections?utm_source=verkorte_url&utm_medium=redirect  
https://www.sentinel-hub.com/ 
https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2 
https://harvardforest.fas.harvard.edu/data-archives/data-archive/ 
https://app1.icgc.cat/web/en/sismologia_sismograma_v2.php?dia=active 